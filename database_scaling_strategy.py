#!/usr/bin/env python3
"""
Database Scaling Strategy for HomeNetMon
Prepares the database for production scaling with high data volumes
"""

import os
import sys
import json
import time
import sqlite3
import threading
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict

class DatabaseScalingStrategy:
    def __init__(self, db_path="homeNetMon.db", project_path=None):
        self.db_path = Path(db_path)
        self.project_path = Path(project_path or Path.cwd())
        self.scaling_recommendations = []
        self.performance_baselines = {}

        # Color codes
        self.colors = {
            'red': '\033[91m',
            'green': '\033[92m',
            'yellow': '\033[93m',
            'blue': '\033[94m',
            'purple': '\033[95m',
            'cyan': '\033[96m',
            'reset': '\033[0m'
        }

    def log_strategy(self, level, category, strategy, details=""):
        """Log scaling strategy recommendation"""
        colors = {
            'critical': self.colors['red'],
            'high': self.colors['yellow'],
            'medium': self.colors['blue'],
            'implemented': self.colors['green'],
            'info': self.colors['cyan']
        }

        icons = {
            'critical': 'üö®',
            'high': '‚ö†Ô∏è',
            'medium': 'üìã',
            'implemented': '‚úÖ',
            'info': '‚ÑπÔ∏è'
        }

        color = colors.get(level, self.colors['blue'])
        icon = icons.get(level, '‚ÑπÔ∏è')

        print(f"{color}{icon} {category}: {strategy}{self.colors['reset']}")
        if details:
            print(f"    ‚îî‚îÄ {details}")

        self.scaling_recommendations.append({
            'priority': level,
            'category': category,
            'strategy': strategy,
            'details': details,
            'timestamp': datetime.now().isoformat()
        })

    def analyze_data_growth_patterns(self):
        """Analyze data growth patterns to predict scaling needs"""
        print(f"\n{self.colors['cyan']}üìà Analyzing Data Growth Patterns{self.colors['reset']}")

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Analyze bandwidth_data growth (largest table)
                cursor.execute("""
                    SELECT
                        DATE(timestamp) as date,
                        COUNT(*) as records_per_day
                    FROM bandwidth_data
                    WHERE timestamp >= datetime('now', '-7 days')
                    GROUP BY DATE(timestamp)
                    ORDER BY date DESC
                """)
                bandwidth_growth = cursor.fetchall()

                if bandwidth_growth:
                    daily_records = [row[1] for row in bandwidth_growth]
                    avg_daily_records = sum(daily_records) / len(daily_records)
                    max_daily_records = max(daily_records)

                    self.log_strategy('info', 'Growth Analysis',
                                    f'Bandwidth data: {avg_daily_records:.0f} records/day average')

                    # Project growth
                    monthly_projection = avg_daily_records * 30
                    yearly_projection = avg_daily_records * 365

                    if yearly_projection > 10000000:  # 10M records/year
                        self.log_strategy('critical', 'Scaling Alert',
                                        f'Projected {yearly_projection:.0f} records/year',
                                        'Requires aggressive archival strategy')
                    elif yearly_projection > 5000000:  # 5M records/year
                        self.log_strategy('high', 'Scaling Alert',
                                        f'Projected {yearly_projection:.0f} records/year',
                                        'Plan for data partitioning')

                # Analyze performance_metrics growth
                cursor.execute("""
                    SELECT COUNT(*) as total_records,
                           MIN(timestamp) as oldest,
                           MAX(timestamp) as newest
                    FROM performance_metrics
                    WHERE timestamp IS NOT NULL
                """)
                perf_stats = cursor.fetchone()

                if perf_stats and perf_stats[0] > 0:
                    total_records, oldest, newest = perf_stats
                    if oldest and newest:
                        # Calculate time span and growth rate
                        oldest_dt = datetime.fromisoformat(oldest.replace('T', ' '))
                        newest_dt = datetime.fromisoformat(newest.replace('T', ' '))
                        days_span = (newest_dt - oldest_dt).days

                        if days_span > 0:
                            records_per_day = total_records / days_span
                            self.log_strategy('info', 'Growth Analysis',
                                            f'Performance metrics: {records_per_day:.0f} records/day')

        except Exception as e:
            self.log_strategy('medium', 'Growth Analysis', f'Analysis error: {e}')

    def create_database_partitioning_strategy(self):
        """Create database partitioning strategy for large tables"""
        print(f"\n{self.colors['cyan']}üóÇÔ∏è Creating Database Partitioning Strategy{self.colors['reset']}")

        partitioning_code = '''"""
Database Partitioning Strategy for HomeNetMon
Implements time-based partitioning for large tables to improve performance
"""

import sqlite3
import threading
from datetime import datetime, timedelta
from pathlib import Path

class DatabasePartitionManager:
    """Manages database partitioning for large tables"""

    def __init__(self, db_path):
        self.db_path = db_path
        self.lock = threading.Lock()

    def create_partition_table(self, base_table, partition_suffix, start_date, end_date):
        """Create a partition table for a specific time range"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Get original table structure
                cursor.execute(f"SELECT sql FROM sqlite_master WHERE type='table' AND name='{base_table}'")
                create_sql = cursor.fetchone()[0]

                # Create partition table
                partition_table = f"{base_table}_{partition_suffix}"
                partition_sql = create_sql.replace(f'CREATE TABLE {base_table}',
                                                 f'CREATE TABLE IF NOT EXISTS {partition_table}')
                cursor.execute(partition_sql)

                # Create time-based check constraint (SQLite doesn't enforce but documents intent)
                print(f"Created partition: {partition_table} for {start_date} to {end_date}")
                return partition_table

        except Exception as e:
            print(f"Partition creation failed: {e}")
            return None

    def migrate_data_to_partitions(self, table, date_column='timestamp'):
        """Migrate existing data to monthly partitions"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Get date range of data
                cursor.execute(f"""
                    SELECT
                        MIN(DATE({date_column})) as min_date,
                        MAX(DATE({date_column})) as max_date
                    FROM {table}
                """)
                date_range = cursor.fetchone()

                if not date_range[0]:
                    return

                min_date = datetime.strptime(date_range[0], '%Y-%m-%d')
                max_date = datetime.strptime(date_range[1], '%Y-%m-%d')

                # Create monthly partitions
                current_date = min_date.replace(day=1)  # Start of month

                while current_date <= max_date:
                    # Calculate partition boundaries
                    next_month = current_date.replace(day=28) + timedelta(days=4)
                    next_month = next_month.replace(day=1)

                    partition_suffix = current_date.strftime('%Y_%m')

                    # Create partition table
                    partition_table = self.create_partition_table(
                        table, partition_suffix,
                        current_date.strftime('%Y-%m-%d'),
                        next_month.strftime('%Y-%m-%d')
                    )

                    if partition_table:
                        # Move data to partition
                        cursor.execute(f"""
                            INSERT INTO {partition_table}
                            SELECT * FROM {table}
                            WHERE DATE({date_column}) >= '{current_date.strftime('%Y-%m-%d')}'
                            AND DATE({date_column}) < '{next_month.strftime('%Y-%m-%d')}'
                        """)

                        # Delete moved data from main table
                        cursor.execute(f"""
                            DELETE FROM {table}
                            WHERE DATE({date_column}) >= '{current_date.strftime('%Y-%m-%d')}'
                            AND DATE({date_column}) < '{next_month.strftime('%Y-%m-%d')}'
                        """)

                        moved_count = cursor.rowcount
                        print(f"Moved {moved_count} records to {partition_table}")

                    current_date = next_month

                conn.commit()

        except Exception as e:
            print(f"Data migration failed: {e}")

    def create_partition_view(self, base_table, partitions):
        """Create a view that unions all partitions for transparent access"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Drop existing view
                cursor.execute(f"DROP VIEW IF EXISTS {base_table}_unified")

                # Create union view
                union_parts = []
                for partition in partitions:
                    union_parts.append(f"SELECT * FROM {partition}")

                view_sql = f"""
                    CREATE VIEW {base_table}_unified AS
                    {' UNION ALL '.join(union_parts)}
                """

                cursor.execute(view_sql)
                print(f"Created unified view: {base_table}_unified")

        except Exception as e:
            print(f"View creation failed: {e}")

# Usage example for bandwidth_data table
def setup_bandwidth_data_partitioning():
    """Set up partitioning for bandwidth_data table"""
    manager = DatabasePartitionManager('homeNetMon.db')

    # Create monthly partitions for bandwidth_data
    manager.migrate_data_to_partitions('bandwidth_data', 'timestamp')

    # Get list of created partitions
    with sqlite3.connect('homeNetMon.db') as conn:
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'bandwidth_data_%'")
        partitions = [row[0] for row in cursor.fetchall()]

    # Create unified view
    if partitions:
        manager.create_partition_view('bandwidth_data', partitions)

if __name__ == "__main__":
    setup_bandwidth_data_partitioning()
'''

        partition_file = self.project_path / "database_partitioning.py"
        with open(partition_file, 'w') as f:
            f.write(partitioning_code)

        self.log_strategy('implemented', 'Partitioning', 'Created database partitioning system',
                         f"Saved to {partition_file}")

    def create_automated_archival_system(self):
        """Create automated data archival system"""
        print(f"\n{self.colors['cyan']}üîÑ Creating Automated Archival System{self.colors['reset']}")

        archival_code = '''"""
Automated Data Archival System for HomeNetMon
Implements scheduled data archival to prevent database bloat
"""

import sqlite3
import time
import schedule
import logging
from datetime import datetime, timedelta
from pathlib import Path

class AutomatedArchivalSystem:
    """Manages automated data archival and cleanup"""

    def __init__(self, db_path, archive_dir="data_archives"):
        self.db_path = Path(db_path)
        self.archive_dir = Path(archive_dir)
        self.archive_dir.mkdir(exist_ok=True)

        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.archive_dir / 'archival.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

        # Archive policies (days to retain)
        self.policies = {
            'bandwidth_data': 30,
            'performance_metrics': 60,
            'monitoring_data': 90,
            'notification_history': 180,
            'security_events': 365,
            'alerts': 365
        }

    def archive_table_data(self, table, retention_days):
        """Archive data from a table older than retention period"""
        try:
            cutoff_date = datetime.now() - timedelta(days=retention_days)
            archive_date = datetime.now().strftime('%Y%m%d')

            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Check if table exists
                cursor.execute(f"SELECT name FROM sqlite_master WHERE type='table' AND name='{table}'")
                if not cursor.fetchone():
                    self.logger.warning(f"Table {table} not found")
                    return 0

                # Find timestamp column
                cursor.execute(f"PRAGMA table_info({table})")
                columns = cursor.fetchall()
                timestamp_col = None

                for col in columns:
                    col_name = col[1]
                    if col_name in ['timestamp', 'created_at', 'time', 'date']:
                        timestamp_col = col_name
                        break

                if not timestamp_col:
                    self.logger.warning(f"No timestamp column found in {table}")
                    return 0

                # Count records to archive
                cursor.execute(f"""
                    SELECT COUNT(*) FROM {table}
                    WHERE {timestamp_col} < ?
                """, (cutoff_date.isoformat(),))

                archive_count = cursor.fetchone()[0]

                if archive_count == 0:
                    self.logger.info(f"No old data to archive in {table}")
                    return 0

                # Create archive table
                archive_table = f"{table}_archive_{archive_date}"
                cursor.execute(f"SELECT sql FROM sqlite_master WHERE type='table' AND name='{table}'")
                create_sql = cursor.fetchone()[0]
                archive_sql = create_sql.replace(f'CREATE TABLE {table}',
                                               f'CREATE TABLE IF NOT EXISTS {archive_table}')
                cursor.execute(archive_sql)

                # Archive old data
                cursor.execute(f"""
                    INSERT INTO {archive_table}
                    SELECT * FROM {table}
                    WHERE {timestamp_col} < ?
                """, (cutoff_date.isoformat(),))

                # Delete archived data from main table
                cursor.execute(f"""
                    DELETE FROM {table}
                    WHERE {timestamp_col} < ?
                """, (cutoff_date.isoformat(),))

                conn.commit()

                self.logger.info(f"Archived {archive_count} records from {table} to {archive_table}")
                return archive_count

        except Exception as e:
            self.logger.error(f"Archival failed for {table}: {e}")
            return 0

    def run_daily_archival(self):
        """Run daily archival process for all tables"""
        self.logger.info("Starting daily archival process")

        total_archived = 0
        for table, retention_days in self.policies.items():
            archived = self.archive_table_data(table, retention_days)
            total_archived += archived

        self.logger.info(f"Daily archival completed: {total_archived} records archived")

        # Run VACUUM if significant data was archived
        if total_archived > 10000:
            self.vacuum_database()

    def vacuum_database(self):
        """Run VACUUM to reclaim space after archival"""
        try:
            self.logger.info("Running database VACUUM to reclaim space")

            with sqlite3.connect(self.db_path) as conn:
                # Get size before VACUUM
                cursor = conn.cursor()
                cursor.execute("PRAGMA page_count")
                pages_before = cursor.fetchone()[0]

                # Run VACUUM
                conn.execute("VACUUM")

                # Get size after VACUUM
                cursor.execute("PRAGMA page_count")
                pages_after = cursor.fetchone()[0]

                space_reclaimed = (pages_before - pages_after) * 4096  # 4KB page size
                space_reclaimed_mb = space_reclaimed / (1024 * 1024)

                self.logger.info(f"VACUUM completed: {space_reclaimed_mb:.1f}MB reclaimed")

        except Exception as e:
            self.logger.error(f"VACUUM failed: {e}")

    def cleanup_old_archives(self, archive_retention_days=365):
        """Clean up archive tables older than specified days"""
        try:
            cutoff_date = datetime.now() - timedelta(days=archive_retention_days)

            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Find archive tables older than cutoff
                cursor.execute("""
                    SELECT name FROM sqlite_master
                    WHERE type='table' AND name LIKE '%_archive_%'
                """)

                archive_tables = cursor.fetchall()
                dropped_count = 0

                for table_name in archive_tables:
                    table = table_name[0]

                    # Extract date from table name (format: table_archive_YYYYMMDD)
                    try:
                        date_part = table.split('_archive_')[-1]
                        archive_date = datetime.strptime(date_part, '%Y%m%d')

                        if archive_date < cutoff_date:
                            cursor.execute(f"DROP TABLE {table}")
                            self.logger.info(f"Dropped old archive table: {table}")
                            dropped_count += 1

                    except ValueError:
                        # Skip tables that don't match expected format
                        continue

                conn.commit()
                self.logger.info(f"Cleaned up {dropped_count} old archive tables")

        except Exception as e:
            self.logger.error(f"Archive cleanup failed: {e}")

    def setup_scheduled_archival(self):
        """Set up scheduled archival tasks"""
        # Daily archival at 2 AM
        schedule.every().day.at("02:00").do(self.run_daily_archival)

        # Weekly archive cleanup on Sundays at 3 AM
        schedule.every().sunday.at("03:00").do(self.cleanup_old_archives)

        self.logger.info("Scheduled archival tasks configured")

    def run_archival_daemon(self):
        """Run the archival system as a daemon"""
        self.setup_scheduled_archival()

        self.logger.info("Starting automated archival daemon")

        while True:
            schedule.run_pending()
            time.sleep(60)  # Check every minute

# Command line interface
if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "daemon":
        # Run as daemon
        archival = AutomatedArchivalSystem('homeNetMon.db')
        archival.run_archival_daemon()
    else:
        # Run once
        archival = AutomatedArchivalSystem('homeNetMon.db')
        archival.run_daily_archival()
'''

        archival_file = self.project_path / "automated_archival_system.py"
        with open(archival_file, 'w') as f:
            f.write(archival_code)

        self.log_strategy('implemented', 'Archival', 'Created automated archival system',
                         f"Saved to {archival_file}")

    def create_database_monitoring_system(self):
        """Create database monitoring and alerting system"""
        print(f"\n{self.colors['cyan']}üìä Creating Database Monitoring System{self.colors['reset']}")

        monitoring_code = '''"""
Database Monitoring and Alerting System for HomeNetMon
Monitors database health, performance, and growth patterns
"""

import sqlite3
import psutil
import time
import json
import smtplib
from datetime import datetime, timedelta
from pathlib import Path
from email.mime.text import MIMEText

class DatabaseMonitor:
    """Monitors database health and performance metrics"""

    def __init__(self, db_path, alert_config=None):
        self.db_path = Path(db_path)
        self.alert_config = alert_config or {}

        # Default thresholds
        self.thresholds = {
            'db_size_mb': 800,          # Alert if DB > 800MB
            'table_rows': 1000000,      # Alert if table > 1M rows
            'query_time_ms': 1000,      # Alert if queries > 1s
            'disk_usage_percent': 85,   # Alert if disk > 85%
            'growth_rate_percent': 50   # Alert if growth > 50% in 24h
        }

    def check_database_size(self):
        """Monitor database size"""
        try:
            db_size_bytes = self.db_path.stat().st_size
            db_size_mb = db_size_bytes / (1024 * 1024)

            status = {
                'metric': 'database_size',
                'value': db_size_mb,
                'unit': 'MB',
                'threshold': self.thresholds['db_size_mb'],
                'status': 'OK' if db_size_mb < self.thresholds['db_size_mb'] else 'ALERT',
                'timestamp': datetime.now().isoformat()
            }

            return status

        except Exception as e:
            return {'metric': 'database_size', 'error': str(e)}

    def check_table_sizes(self):
        """Monitor individual table sizes"""
        results = []

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Get all table names
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'")
                tables = [row[0] for row in cursor.fetchall()]

                for table in tables:
                    try:
                        cursor.execute(f"SELECT COUNT(*) FROM {table}")
                        row_count = cursor.fetchone()[0]

                        status = {
                            'metric': f'table_size_{table}',
                            'value': row_count,
                            'unit': 'rows',
                            'threshold': self.thresholds['table_rows'],
                            'status': 'OK' if row_count < self.thresholds['table_rows'] else 'ALERT',
                            'timestamp': datetime.now().isoformat()
                        }

                        results.append(status)

                    except Exception as e:
                        results.append({
                            'metric': f'table_size_{table}',
                            'error': str(e)
                        })

        except Exception as e:
            results.append({'metric': 'table_sizes', 'error': str(e)})

        return results

    def check_query_performance(self):
        """Monitor query performance"""
        test_queries = [
            ("SELECT COUNT(*) FROM devices", "device_count"),
            ("SELECT COUNT(*) FROM monitoring_data", "monitoring_count"),
            ("SELECT * FROM devices ORDER BY last_seen DESC LIMIT 10", "recent_devices")
        ]

        results = []

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                for query, name in test_queries:
                    try:
                        start_time = time.time()
                        cursor.execute(query)
                        cursor.fetchall()
                        query_time_ms = (time.time() - start_time) * 1000

                        status = {
                            'metric': f'query_performance_{name}',
                            'value': query_time_ms,
                            'unit': 'ms',
                            'threshold': self.thresholds['query_time_ms'],
                            'status': 'OK' if query_time_ms < self.thresholds['query_time_ms'] else 'ALERT',
                            'timestamp': datetime.now().isoformat()
                        }

                        results.append(status)

                    except Exception as e:
                        results.append({
                            'metric': f'query_performance_{name}',
                            'error': str(e)
                        })

        except Exception as e:
            results.append({'metric': 'query_performance', 'error': str(e)})

        return results

    def check_disk_usage(self):
        """Monitor disk usage"""
        try:
            disk_usage = psutil.disk_usage(self.db_path.parent)
            usage_percent = (disk_usage.used / disk_usage.total) * 100

            status = {
                'metric': 'disk_usage',
                'value': usage_percent,
                'unit': 'percent',
                'threshold': self.thresholds['disk_usage_percent'],
                'status': 'OK' if usage_percent < self.thresholds['disk_usage_percent'] else 'ALERT',
                'timestamp': datetime.now().isoformat()
            }

            return status

        except Exception as e:
            return {'metric': 'disk_usage', 'error': str(e)}

    def check_growth_rate(self):
        """Monitor database growth rate"""
        try:
            # This would require historical size data
            # For now, return a placeholder
            status = {
                'metric': 'growth_rate',
                'value': 0,
                'unit': 'percent_per_day',
                'threshold': self.thresholds['growth_rate_percent'],
                'status': 'OK',
                'timestamp': datetime.now().isoformat(),
                'note': 'Requires historical data collection'
            }

            return status

        except Exception as e:
            return {'metric': 'growth_rate', 'error': str(e)}

    def run_health_check(self):
        """Run complete database health check"""
        health_report = {
            'timestamp': datetime.now().isoformat(),
            'database_path': str(self.db_path),
            'checks': []
        }

        # Run all checks
        health_report['checks'].append(self.check_database_size())
        health_report['checks'].extend(self.check_table_sizes())
        health_report['checks'].extend(self.check_query_performance())
        health_report['checks'].append(self.check_disk_usage())
        health_report['checks'].append(self.check_growth_rate())

        # Count alerts
        alerts = [check for check in health_report['checks']
                 if check.get('status') == 'ALERT']

        health_report['summary'] = {
            'total_checks': len(health_report['checks']),
            'alerts': len(alerts),
            'overall_status': 'ALERT' if alerts else 'OK'
        }

        return health_report

    def send_alert(self, health_report):
        """Send alert if issues detected"""
        if health_report['summary']['overall_status'] != 'ALERT':
            return

        # Email configuration from alert_config
        if not self.alert_config.get('email'):
            print("Alert detected but email not configured")
            return

        try:
            alerts = [check for check in health_report['checks']
                     if check.get('status') == 'ALERT']

            subject = f"HomeNetMon Database Alert - {len(alerts)} issues detected"

            body = f"""
Database Health Alert

Timestamp: {health_report['timestamp']}
Database: {health_report['database_path']}

Alerts Detected:
"""

            for alert in alerts:
                body += f"- {alert['metric']}: {alert['value']} {alert['unit']} (threshold: {alert['threshold']})\n"

            msg = MIMEText(body)
            msg['Subject'] = subject
            msg['From'] = self.alert_config['email']['from']
            msg['To'] = self.alert_config['email']['to']

            # Send email
            server = smtplib.SMTP(self.alert_config['email']['smtp_server'])
            server.send_message(msg)
            server.quit()

            print(f"Alert email sent for {len(alerts)} issues")

        except Exception as e:
            print(f"Failed to send alert: {e}")

# Usage example
if __name__ == "__main__":
    monitor = DatabaseMonitor('homeNetMon.db')
    health_report = monitor.run_health_check()

    print(json.dumps(health_report, indent=2))

    if health_report['summary']['alerts'] > 0:
        print(f"\\nALERT: {health_report['summary']['alerts']} issues detected!")
'''

        monitoring_file = self.project_path / "database_monitoring.py"
        with open(monitoring_file, 'w') as f:
            f.write(monitoring_code)

        self.log_strategy('implemented', 'Monitoring', 'Created database monitoring system',
                         f"Saved to {monitoring_file}")

    def create_scaling_configuration(self):
        """Create configuration for database scaling"""
        print(f"\n{self.colors['cyan']}‚öôÔ∏è Creating Scaling Configuration{self.colors['reset']}")

        scaling_config = {
            "database_scaling": {
                "current_setup": "SQLite single file",
                "recommended_production": "PostgreSQL with connection pooling",
                "scaling_strategy": {
                    "phase_1": {
                        "description": "Optimize current SQLite setup",
                        "actions": [
                            "Implement connection pooling",
                            "Add response caching",
                            "Enable WAL mode",
                            "Regular VACUUM operations"
                        ],
                        "capacity": "Up to 100 concurrent users"
                    },
                    "phase_2": {
                        "description": "Advanced SQLite optimizations",
                        "actions": [
                            "Implement table partitioning",
                            "Automated data archival",
                            "Read replicas for analytics",
                            "Database monitoring alerts"
                        ],
                        "capacity": "Up to 500 concurrent users"
                    },
                    "phase_3": {
                        "description": "Migrate to PostgreSQL",
                        "actions": [
                            "Set up PostgreSQL cluster",
                            "Implement read/write split",
                            "Advanced caching (Redis)",
                            "Horizontal scaling"
                        ],
                        "capacity": "1000+ concurrent users"
                    }
                },
                "data_management": {
                    "retention_policies": {
                        "bandwidth_data": "30 days active, 12 months archived",
                        "performance_metrics": "60 days active, 24 months archived",
                        "monitoring_data": "90 days active, 12 months archived",
                        "alerts": "365 days active, 36 months archived"
                    },
                    "archival_schedule": "Daily at 2 AM",
                    "backup_schedule": "Every 6 hours",
                    "cleanup_schedule": "Weekly on Sundays"
                },
                "performance_targets": {
                    "query_response_time": "< 100ms for 95% of queries",
                    "database_size": "< 500MB active data",
                    "concurrent_connections": "100+ simultaneous",
                    "availability": "99.9% uptime"
                },
                "monitoring_thresholds": {
                    "database_size_mb": 800,
                    "table_row_limit": 1000000,
                    "query_timeout_ms": 1000,
                    "disk_usage_percent": 85,
                    "memory_usage_mb": 1000
                }
            }
        }

        config_file = self.project_path / "database_scaling_config.json"
        with open(config_file, 'w') as f:
            json.dump(scaling_config, f, indent=2)

        self.log_strategy('implemented', 'Configuration', 'Created scaling configuration',
                         f"Saved to {config_file}")

    def benchmark_current_performance(self):
        """Benchmark current database performance as baseline"""
        print(f"\n{self.colors['cyan']}üèÉ Benchmarking Current Performance{self.colors['reset']}")

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Test concurrent connections simulation
                benchmark_results = {}

                # Single query performance
                queries = [
                    ("SELECT COUNT(*) FROM devices", "device_count"),
                    ("SELECT COUNT(*) FROM monitoring_data", "monitoring_count"),
                    ("SELECT * FROM devices ORDER BY last_seen DESC LIMIT 100", "recent_devices"),
                    ("SELECT device_id, AVG(response_time) FROM monitoring_data GROUP BY device_id LIMIT 50", "device_averages")
                ]

                for query, name in queries:
                    times = []
                    for i in range(5):  # Run 5 times
                        start_time = time.time()
                        cursor.execute(query)
                        cursor.fetchall()
                        times.append((time.time() - start_time) * 1000)

                    avg_time = sum(times) / len(times)
                    benchmark_results[name] = {
                        'avg_time_ms': avg_time,
                        'min_time_ms': min(times),
                        'max_time_ms': max(times)
                    }

                # Database size info
                db_size_mb = self.db_path.stat().st_size / (1024 * 1024)
                benchmark_results['database_size_mb'] = db_size_mb

                self.performance_baselines = benchmark_results

                for query_name, metrics in benchmark_results.items():
                    if isinstance(metrics, dict) and 'avg_time_ms' in metrics:
                        if metrics['avg_time_ms'] < 50:
                            level = 'implemented'
                        elif metrics['avg_time_ms'] < 200:
                            level = 'medium'
                        else:
                            level = 'high'

                        self.log_strategy(level, 'Baseline Performance',
                                        f"{query_name}: {metrics['avg_time_ms']:.2f}ms average")

        except Exception as e:
            self.log_strategy('high', 'Benchmarking', f'Performance test failed: {e}')

    def generate_scaling_recommendations(self):
        """Generate specific scaling recommendations based on analysis"""
        print(f"\n{self.colors['cyan']}üìã Generating Scaling Recommendations{self.colors['reset']}")

        # Immediate recommendations
        self.log_strategy('high', 'Immediate Action',
                         'Implement connection pooling',
                         'Critical for handling concurrent users')

        self.log_strategy('high', 'Immediate Action',
                         'Add response caching for expensive queries',
                         'Will reduce database load significantly')

        self.log_strategy('medium', 'Short Term',
                         'Set up automated daily archival',
                         'Prevent database from growing beyond 1GB')

        self.log_strategy('medium', 'Short Term',
                         'Implement table partitioning for bandwidth_data',
                         'Improve query performance on large datasets')

        # Long term recommendations
        self.log_strategy('medium', 'Long Term',
                         'Consider PostgreSQL migration',
                         'For > 500 concurrent users or > 10GB data')

        self.log_strategy('info', 'Monitoring',
                         'Set up database health monitoring',
                         'Proactive alerting for performance issues')

    def generate_scaling_report(self):
        """Generate comprehensive scaling strategy report"""
        print(f"\n{self.colors['purple']}üìä Database Scaling Strategy Report{self.colors['reset']}")
        print("=" * 80)

        print(f"\nüéØ Current Status:")
        print(f"  Database Size: {self.db_path.stat().st_size / (1024 * 1024):.1f}MB")
        print(f"  Optimization Level: Partially Optimized")
        print(f"  Scaling Readiness: Phase 1 (SQLite Optimizations)")

        print(f"\nüìà Scaling Strategy:")
        print(f"  Phase 1: SQLite Optimization (Current)")
        print(f"    ‚Ä¢ Connection pooling ‚úÖ")
        print(f"    ‚Ä¢ Response caching ‚úÖ")
        print(f"    ‚Ä¢ Database optimization ‚úÖ")
        print(f"    ‚Ä¢ Automated archival üîÑ")

        print(f"\n  Phase 2: Advanced SQLite (Next)")
        print(f"    ‚Ä¢ Table partitioning")
        print(f"    ‚Ä¢ Read replicas")
        print(f"    ‚Ä¢ Advanced monitoring")

        print(f"\n  Phase 3: PostgreSQL Migration (Future)")
        print(f"    ‚Ä¢ Horizontal scaling")
        print(f"    ‚Ä¢ Read/write splitting")
        print(f"    ‚Ä¢ Enterprise features")

        print(f"\nüí° Immediate Actions Required:")
        high_priority = [rec for rec in self.scaling_recommendations if rec['priority'] == 'high']
        for i, rec in enumerate(high_priority[:5], 1):
            print(f"  {i}. {rec['strategy']}")

        print(f"\nüìä Performance Baseline:")
        if self.performance_baselines:
            for metric, value in self.performance_baselines.items():
                if isinstance(value, dict):
                    print(f"  {metric}: {value['avg_time_ms']:.2f}ms average")

        print(f"\n‚è∞ Strategy Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    def save_scaling_strategy(self):
        """Save scaling strategy to file"""
        strategy_data = {
            'timestamp': datetime.now().isoformat(),
            'database_path': str(self.db_path),
            'current_size_mb': self.db_path.stat().st_size / (1024 * 1024),
            'performance_baselines': self.performance_baselines,
            'scaling_recommendations': self.scaling_recommendations,
            'implementation_priority': [
                rec for rec in self.scaling_recommendations
                if rec['priority'] in ['critical', 'high']
            ]
        }

        strategy_file = self.project_path / "database_scaling_strategy.json"
        with open(strategy_file, 'w') as f:
            json.dump(strategy_data, f, indent=2, default=str)

        self.log_strategy('implemented', 'Documentation', 'Scaling strategy saved',
                         f"Saved to {strategy_file}")

def main():
    """Main function to create database scaling strategy"""
    print(f"üöÄ Creating Database Scaling Strategy for HomeNetMon")
    print(f"üìä Target: Prepare for production scaling")
    print(f"‚è∞ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    strategy = DatabaseScalingStrategy()

    # Run all strategy components
    strategy.analyze_data_growth_patterns()
    strategy.create_database_partitioning_strategy()
    strategy.create_automated_archival_system()
    strategy.create_database_monitoring_system()
    strategy.create_scaling_configuration()
    strategy.benchmark_current_performance()
    strategy.generate_scaling_recommendations()

    # Generate final report
    strategy.generate_scaling_report()
    strategy.save_scaling_strategy()

    print(f"\nüéâ Database scaling strategy completed!")

if __name__ == "__main__":
    main()